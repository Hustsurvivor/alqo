train LeroModelPairWise
Epoch: 0, training loss: 0.6693120240347324
Epoch: 1, training loss: 0.6310311403413623
Epoch: 2, training loss: 0.6117361394985926
Epoch: 3, training loss: 0.58849030482882
Epoch: 4, training loss: 0.5778534271869672
Epoch: 5, training loss: 0.5574476338076525
Epoch: 6, training loss: 0.5410122212390608
Epoch: 7, training loss: 0.5372582544442266
Epoch: 8, training loss: 0.48901231615893637
Epoch: 9, training loss: 0.4929434678885966
Epoch: 10, training loss: 0.45793463103906834
Epoch: 11, training loss: 0.4495398494422809
Epoch: 12, training loss: 0.43372763611819615
Epoch: 13, training loss: 0.4241300440361955
Epoch: 14, training loss: 0.40576587765788324
Epoch: 15, training loss: 0.4212893622921505
Epoch: 16, training loss: 0.38782203109267566
Epoch: 17, training loss: 0.37877238826096976
Epoch: 18, training loss: 0.3788288237153825
Epoch: 19, training loss: 0.3476625254035988
Epoch: 20, training loss: 0.3571759516630793
Epoch: 21, training loss: 0.3472026323269655
Epoch: 22, training loss: 0.3529645074431693
Epoch: 23, training loss: 0.3277024380266173
Epoch: 24, training loss: 0.32942723295219584
Epoch: 25, training loss: 0.32594001644788434
Epoch: 26, training loss: 0.30919070722203906
Epoch: 27, training loss: 0.31323795988823544
Epoch: 28, training loss: 0.29219598364919813
Epoch: 29, training loss: 0.311166271607072
Epoch: 30, training loss: 0.30689733669502345
Epoch: 31, training loss: 0.2794954219201636
Epoch: 32, training loss: 0.28836540713141445
Epoch: 33, training loss: 0.28441067582105933
Epoch: 34, training loss: 0.26317067544044725
Epoch: 35, training loss: 0.26668010016386073
Epoch: 36, training loss: 0.2710360086701123
Epoch: 37, training loss: 0.2540934033018319
Epoch: 38, training loss: 0.23502632740827542
Epoch: 39, training loss: 0.23738202632752267
Epoch: 40, training loss: 0.26192919554869243
Epoch: 41, training loss: 0.2316509017122267
Epoch: 42, training loss: 0.25841171617247155
Epoch: 43, training loss: 0.2384217602627291
Epoch: 44, training loss: 0.23656585757929138
Epoch: 45, training loss: 0.2293060445936603
Epoch: 46, training loss: 0.2219157495821996
Epoch: 47, training loss: 0.21873694761897386
Epoch: 48, training loss: 0.21319556054409253
Epoch: 49, training loss: 0.22258387955155934
Epoch: 50, training loss: 0.21253421762491762
Epoch: 51, training loss: 0.22013586497076312
Epoch: 52, training loss: 0.20784907720883636
Epoch: 53, training loss: 0.20740153050735585
Epoch: 54, training loss: 0.20919362173477155
Epoch: 55, training loss: 0.2095913223988064
Epoch: 56, training loss: 0.20106813938058407
Epoch: 57, training loss: 0.21104908598297803
Epoch: 58, training loss: 0.21883600523702515
Epoch: 59, training loss: 0.18679651045299472
Epoch: 60, training loss: 0.21307603963843913
Epoch: 61, training loss: 0.19608395808994272
Epoch: 62, training loss: 0.181811451870301
Epoch: 63, training loss: 0.19122602384764834
Epoch: 64, training loss: 0.19668377075097218
Epoch: 65, training loss: 0.18897582542367733
Epoch: 66, training loss: 0.1810415267212221
Epoch: 67, training loss: 0.17627530253710375
Epoch: 68, training loss: 0.20135563706726115
Epoch: 69, training loss: 0.18202683706166048
Epoch: 70, training loss: 0.1687094715475692
Epoch: 71, training loss: 0.18796315363844793
Epoch: 72, training loss: 0.17086470875983453
Epoch: 73, training loss: 0.16957733765364572
Epoch: 74, training loss: 0.16947715453992782
Epoch: 75, training loss: 0.18394592022127035
Epoch: 76, training loss: 0.17163079702821646
Epoch: 77, training loss: 0.17670219458576794
Epoch: 78, training loss: 0.17222591288585812
Epoch: 79, training loss: 0.16598439891330588
Epoch: 80, training loss: 0.204117292529542
Epoch: 81, training loss: 0.16977240780243527
Epoch: 82, training loss: 0.1364721481646234
Epoch: 83, training loss: 0.15216632379906994
Epoch: 84, training loss: 0.15471761699663197
Epoch: 85, training loss: 0.16286329908681207
Epoch: 86, training loss: 0.16466228510218608
Epoch: 87, training loss: 0.16022021998265393
Epoch: 88, training loss: 0.16750840635455097
Epoch: 89, training loss: 0.15781628119217125
Epoch: 90, training loss: 0.1223373796292576
Epoch: 91, training loss: 0.10290518958018297
Epoch: 92, training loss: 0.07817686659353959
Epoch: 93, training loss: 0.08608887632699724
Epoch: 94, training loss: 0.07274072840618899
Epoch: 95, training loss: 0.07001968180761481
Epoch: 96, training loss: 0.057589776708318004
Epoch: 97, training loss: 0.05553363227946955
Epoch: 98, training loss: 0.059235858108586135
Epoch: 99, training loss: 0.06235296470727684
Epoch: 100, training loss: 0.06491600552348763
Epoch: 101, training loss: 0.06213244333365745
Epoch: 102, training loss: 0.060269653574562675
Epoch: 103, training loss: 0.05119390055706951
Epoch: 104, training loss: 0.06426571274464968
Epoch: 105, training loss: 0.049778762559286927
Epoch: 106, training loss: 0.05595310365676872
Epoch: 107, training loss: 0.04841413523494875
Epoch: 108, training loss: 0.06312178106142403
Epoch: 109, training loss: 0.056561542176067396
Epoch: 110, training loss: 0.05783121092587578
Epoch: 111, training loss: 0.04904340211431149
Epoch: 112, training loss: 0.05110136090833723
Epoch: 113, training loss: 0.050979009904983774
Epoch: 114, training loss: 0.042504743385912536
Epoch: 115, training loss: 0.045178430452749135
Epoch: 116, training loss: 0.043930016328029814
Epoch: 117, training loss: 0.04251525293612327
Epoch: 118, training loss: 0.04121574743321971
Epoch: 119, training loss: 0.03801973753661401
