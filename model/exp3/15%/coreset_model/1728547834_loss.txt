train BayesianModelPairWise
Epoch: 0, training loss: 0.03892971552484448
Epoch: 1, training loss: 0.03622948166724167
Epoch: 2, training loss: 0.035520096813543706
Epoch: 3, training loss: 0.03517748142051268
Epoch: 4, training loss: 0.034864092229370584
Epoch: 5, training loss: 0.03455161114065179
Epoch: 6, training loss: 0.03432755192041001
Epoch: 7, training loss: 0.03414277380059563
Epoch: 8, training loss: 0.03398308499512461
Epoch: 9, training loss: 0.03384908873422314
Epoch: 10, training loss: 0.033731285065038454
Epoch: 11, training loss: 0.033585737805215024
Epoch: 12, training loss: 0.033470739118196244
Epoch: 13, training loss: 0.03344912710160019
Epoch: 14, training loss: 0.03331899437209713
Epoch: 15, training loss: 0.03321455369246135
Epoch: 16, training loss: 0.03314365885811107
Epoch: 17, training loss: 0.03308255353056336
Epoch: 18, training loss: 0.033022932847390836
Epoch: 19, training loss: 0.03297001557188565
Epoch: 20, training loss: 0.03286424529496264
Epoch: 21, training loss: 0.03280865673157437
Epoch: 22, training loss: 0.03282789417775071
Epoch: 23, training loss: 0.032640979817017496
Epoch: 24, training loss: 0.0325972804059913
Epoch: 25, training loss: 0.03257823025428219
Epoch: 26, training loss: 0.03253034416032195
Epoch: 27, training loss: 0.03250228597626293
Epoch: 28, training loss: 0.03244588478856753
Epoch: 29, training loss: 0.03238501463170561
Epoch: 30, training loss: 0.03235654732477219
Epoch: 31, training loss: 0.03224946955580345
Epoch: 32, training loss: 0.03222728532032648
Epoch: 33, training loss: 0.03216589883931522
Epoch: 34, training loss: 0.0321450709195064
Epoch: 35, training loss: 0.032130801472765855
Epoch: 36, training loss: 0.03208533303599466
Epoch: 37, training loss: 0.032059339865620734
Epoch: 38, training loss: 0.03200515444764527
Epoch: 39, training loss: 0.031979771441491917
Epoch: 40, training loss: 0.031969594546543814
Epoch: 41, training loss: 0.03191714609967972
Epoch: 42, training loss: 0.03191673576916485
Epoch: 43, training loss: 0.03182369453146136
Epoch: 44, training loss: 0.03183985891003938
Epoch: 45, training loss: 0.031775301816370886
Epoch: 46, training loss: 0.031773411829668426
Epoch: 47, training loss: 0.03172903887446806
Epoch: 48, training loss: 0.031639488210244204
Epoch: 49, training loss: 0.03165232513099226
Epoch: 50, training loss: 0.031668436337908744
Epoch: 51, training loss: 0.03161855344798042
Epoch: 52, training loss: 0.03160665147377622
Epoch: 53, training loss: 0.03158926682702234
Epoch: 54, training loss: 0.03150730430496038
Epoch: 55, training loss: 0.031515366310201166
Epoch: 56, training loss: 0.03152028907075861
Epoch: 57, training loss: 0.03148596366368844
Epoch: 58, training loss: 0.031455644576624496
Epoch: 59, training loss: 0.03146385432060515
Epoch: 60, training loss: 0.03140755788166131
Epoch: 61, training loss: 0.031433526412972104
Epoch: 62, training loss: 0.03142632662627309
Epoch: 63, training loss: 0.03137608097147229
Epoch: 64, training loss: 0.03132894191853476
Epoch: 65, training loss: 0.03132710911068251
Epoch: 66, training loss: 0.03131457862634181
Epoch: 67, training loss: 0.031281071295200094
Epoch: 68, training loss: 0.03129509975395959
Epoch: 69, training loss: 0.031218065052057185
Epoch: 70, training loss: 0.031230572240983384
Epoch: 71, training loss: 0.031200513490167107
Epoch: 72, training loss: 0.03120835771904255
Epoch: 73, training loss: 0.031164031225138077
Epoch: 74, training loss: 0.03116877274260135
Epoch: 75, training loss: 0.031143191896128217
Epoch: 76, training loss: 0.031145955610530393
Epoch: 77, training loss: 0.03113171296900984
Epoch: 78, training loss: 0.031092317153460704
Epoch: 79, training loss: 0.031096892485159733
Epoch: 80, training loss: 0.031039019510514146
Epoch: 81, training loss: 0.031017740387288602
Epoch: 82, training loss: 0.03106852505285746
Epoch: 83, training loss: 0.031009814350905954
Epoch: 84, training loss: 0.030996207883176672
Epoch: 85, training loss: 0.030966255302877634
Epoch: 86, training loss: 0.03098454246556805
Epoch: 87, training loss: 0.030966920657097902
Epoch: 88, training loss: 0.030942145293892616
Epoch: 89, training loss: 0.03090734412394358
Epoch: 90, training loss: 0.030869418659634086
Epoch: 91, training loss: 0.030880667571703026
Epoch: 92, training loss: 0.0308827322299034
Epoch: 93, training loss: 0.030878092897423246
Epoch: 94, training loss: 0.03087220063150307
Epoch: 95, training loss: 0.03081593714239445
Epoch: 96, training loss: 0.03084509583405818
Epoch: 97, training loss: 0.030795772010539936
Epoch: 98, training loss: 0.03076594832729251
Epoch: 99, training loss: 0.03078885344646492
