train BayesianModelPairWise
Epoch: 0, training loss: 0.03648081318625858
Epoch: 1, training loss: 0.03317082863794967
Epoch: 2, training loss: 0.03238391746667022
Epoch: 3, training loss: 0.03190295155796653
Epoch: 4, training loss: 0.031604233654087294
Epoch: 5, training loss: 0.03133175056793551
Epoch: 6, training loss: 0.031198645150724232
Epoch: 7, training loss: 0.031027949388819243
Epoch: 8, training loss: 0.030923139637561874
Epoch: 9, training loss: 0.030856804144957294
Epoch: 10, training loss: 0.030683150653701252
Epoch: 11, training loss: 0.030578549054067004
Epoch: 12, training loss: 0.03047909702969244
Epoch: 13, training loss: 0.030408868672554802
Epoch: 14, training loss: 0.030287940876473067
Epoch: 15, training loss: 0.03025386317545811
Epoch: 16, training loss: 0.03020761504673217
Epoch: 17, training loss: 0.030115642599884272
Epoch: 18, training loss: 0.0299914891046307
Epoch: 19, training loss: 0.029944442374966504
Epoch: 20, training loss: 0.029926934123236753
Epoch: 21, training loss: 0.029796513914551247
Epoch: 22, training loss: 0.029760003581515364
Epoch: 23, training loss: 0.029707670121548818
Epoch: 24, training loss: 0.02968525240580409
Epoch: 25, training loss: 0.029598694575611758
Epoch: 26, training loss: 0.02955642772410395
Epoch: 27, training loss: 0.02949768405407749
Epoch: 28, training loss: 0.029479753544678945
Epoch: 29, training loss: 0.029375102617979383
Epoch: 30, training loss: 0.02938157674984227
Epoch: 31, training loss: 0.029315715385444353
Epoch: 32, training loss: 0.029303702499303287
Epoch: 33, training loss: 0.02923283927915226
Epoch: 34, training loss: 0.029179101985433543
Epoch: 35, training loss: 0.029176487574478664
Epoch: 36, training loss: 0.029149014118275036
Epoch: 37, training loss: 0.029046897417850395
Epoch: 38, training loss: 0.029024744750898578
Epoch: 39, training loss: 0.02899798848244279
Epoch: 40, training loss: 0.028980610653212016
Epoch: 41, training loss: 0.028949886209842426
Epoch: 42, training loss: 0.028905523391504824
Epoch: 43, training loss: 0.028885046601453655
Epoch: 44, training loss: 0.028833890790597342
Epoch: 45, training loss: 0.028747658655684384
Epoch: 46, training loss: 0.02880418469454231
Epoch: 47, training loss: 0.028783811800163952
Epoch: 48, training loss: 0.02872866373716828
Epoch: 49, training loss: 0.028737104756340684
Epoch: 50, training loss: 0.028723845902269576
Epoch: 51, training loss: 0.02863822105871589
Epoch: 52, training loss: 0.028643119608096995
Epoch: 53, training loss: 0.028559941795249004
Epoch: 54, training loss: 0.02856563150065461
Epoch: 55, training loss: 0.028566086174654098
Epoch: 56, training loss: 0.028552980654580493
Epoch: 57, training loss: 0.028480907053439888
Epoch: 58, training loss: 0.028432287913043128
Epoch: 59, training loss: 0.028466179519507747
Epoch: 60, training loss: 0.028435385934692886
Epoch: 61, training loss: 0.02841797134782011
Epoch: 62, training loss: 0.02837826399806585
Epoch: 63, training loss: 0.028341568457287847
Epoch: 64, training loss: 0.02832920101903279
Epoch: 65, training loss: 0.0283454227081139
Epoch: 66, training loss: 0.028255385040724886
Epoch: 67, training loss: 0.02830821952487649
Epoch: 68, training loss: 0.0282879576382703
Epoch: 69, training loss: 0.02821605934793976
Epoch: 70, training loss: 0.02824104228252363
Epoch: 71, training loss: 0.028205288121457752
Epoch: 72, training loss: 0.02822321651358723
Epoch: 73, training loss: 0.028191863706433035
Epoch: 74, training loss: 0.028173032629566677
Epoch: 75, training loss: 0.028135180358406606
Epoch: 76, training loss: 0.028126220638488172
Epoch: 77, training loss: 0.02809424981826486
Epoch: 78, training loss: 0.028060362099974485
Epoch: 79, training loss: 0.028087117201889875
Epoch: 80, training loss: 0.028049020801053856
Epoch: 81, training loss: 0.02803526874652895
Epoch: 82, training loss: 0.028008647267962726
Epoch: 83, training loss: 0.02800911326940061
Epoch: 84, training loss: 0.028012748632433276
Epoch: 85, training loss: 0.027986875169764388
Epoch: 86, training loss: 0.02800013504458634
Epoch: 87, training loss: 0.0279498370221503
Epoch: 88, training loss: 0.027923275694156063
Epoch: 89, training loss: 0.027938874123890317
Epoch: 90, training loss: 0.02789623676222149
Epoch: 91, training loss: 0.027889039543186545
Epoch: 92, training loss: 0.027898223819329538
Epoch: 93, training loss: 0.027903211187666296
Epoch: 94, training loss: 0.02791801988910627
Epoch: 95, training loss: 0.0278611437963946
Epoch: 96, training loss: 0.027840124544127386
Epoch: 97, training loss: 0.0278433715264895
Epoch: 98, training loss: 0.027806551682802515
Epoch: 99, training loss: 0.027829182047849347
