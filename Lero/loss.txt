do expriment2:
------------------- finish filter close latency -------------------------
count of origin plan pairs: 54413
count of filtered plan pairs: 44411
-------------------------------------------------------------------------
RelType :  {'Materialize', 'Hash', 'Seq Scan', 'Hash Join', 'Merge Join', 'Nested Loop', 'Index Scan', 'Aggregate', 'Index Only Scan', 'Sort'}
input_feature_dim: 26
Epoch 0 training loss: 0.5275669799735035
Epoch 1 training loss: 0.4655941762003059
Epoch 2 training loss: 0.444687496292596
Epoch 3 training loss: 0.4303623089382392
Epoch 4 training loss: 0.42161281936732564
Epoch 5 training loss: 0.4130890811824697
Epoch 6 training loss: 0.4049733433065634
Epoch 7 training loss: 0.39865265128796645
Epoch 8 training loss: 0.3909216766008628
Epoch 9 training loss: 0.3863703516219995
Epoch 10 training loss: 0.3799226194996634
Epoch 11 training loss: 0.37556678850023417
Epoch 12 training loss: 0.37070666633088656
Epoch 13 training loss: 0.36595144774448984
Epoch 14 training loss: 0.36109148884787623
Epoch 15 training loss: 0.3560977715858834
Epoch 16 training loss: 0.3509515088039889
Epoch 17 training loss: 0.3467284066580005
Epoch 18 training loss: 0.34317421486395006
Epoch 19 training loss: 0.33893333059045133
Epoch 20 training loss: 0.3360422875377466
Epoch 21 training loss: 0.3304657371414786
Epoch 22 training loss: 0.32872573408602257
Epoch 23 training loss: 0.3249206467161039
Epoch 24 training loss: 0.3200460992201337
Epoch 25 training loss: 0.31879655156780456
Epoch 26 training loss: 0.31580588547346355
Epoch 27 training loss: 0.31204262029241114
Epoch 28 training loss: 0.30791595802375427
Epoch 29 training loss: 0.30683726030211017
Epoch 30 training loss: 0.30244301123201334
Epoch 31 training loss: 0.3023979227614365
Epoch 32 training loss: 0.2983694555129538
Epoch 33 training loss: 0.29660615735335494
Epoch 34 training loss: 0.2929120995744102
Epoch 35 training loss: 0.29052633155832425
Epoch 36 training loss: 0.2880510053513454
Epoch 37 training loss: 0.28655330947035257
Epoch 38 training loss: 0.28450503602654764
Epoch 39 training loss: 0.28275138853711534
Epoch 40 training loss: 0.28055507632405985
Epoch 41 training loss: 0.2783035902976919
Epoch 42 training loss: 0.27649562666953764
Epoch 43 training loss: 0.27455249385134506
Epoch 44 training loss: 0.2719395012486985
Epoch 45 training loss: 0.26964290530448737
Epoch 46 training loss: 0.26929358461199243
Epoch 47 training loss: 0.26745508706866356
Epoch 48 training loss: 0.2645046440137219
Epoch 49 training loss: 0.26419299962462023
Epoch 50 training loss: 0.2620684640977759
Epoch 51 training loss: 0.2599563993450941
Epoch 52 training loss: 0.2581307115378776
Epoch 53 training loss: 0.2577674395821114
Epoch 54 training loss: 0.2566841959071547
Epoch 55 training loss: 0.2548718888607755
Epoch 56 training loss: 0.25226951637477707
Epoch 57 training loss: 0.25132312816897295
Epoch 58 training loss: 0.24950330379483782
Epoch 59 training loss: 0.24935931894368332
Epoch 60 training loss: 0.24721916103169106
Epoch 61 training loss: 0.24736449860067494
Epoch 62 training loss: 0.24386748100017216
Epoch 63 training loss: 0.24389099261647418
Epoch 64 training loss: 0.24285625084759338
Epoch 65 training loss: 0.2421298761737183
Epoch 66 training loss: 0.24041547547599762
Epoch 67 training loss: 0.23861710078995282
Epoch 68 training loss: 0.23722565566675982
Epoch 69 training loss: 0.23668029624193426
Epoch 70 training loss: 0.2357888177677418
Epoch 71 training loss: 0.23439909046030497
Epoch 72 training loss: 0.23249307718119747
Epoch 73 training loss: 0.23201965562370772
Epoch 74 training loss: 0.23066052982617566
Epoch 75 training loss: 0.2286063351723128
Epoch 76 training loss: 0.22933072044466177
Epoch 77 training loss: 0.22811188026727128
Epoch 78 training loss: 0.2267617667071548
Epoch 79 training loss: 0.2274034630750151
Epoch 80 training loss: 0.22365877378107316
Epoch 81 training loss: 0.22369465025128582
Epoch 82 training loss: 0.22205710461237613
Epoch 83 training loss: 0.2231579582011527
Epoch 84 training loss: 0.22125607409821957
